{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jtuckayo/scratchpad/blob/main/notebooks/guides/getting-started/v2/cohere_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AoC3UKUyw3K"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cohere-ai/cohere-developer-experience/blob/main/notebooks/guides/getting-started/v2/tutorial_pt3_v2.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-NYFLWjyw3L"
      },
      "source": [
        "# Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN2V6f1hyw3M"
      },
      "source": [
        "As its name implies, the Chat endpoint enables developers to build chatbots that can handle conversations. At the core of a conversation is a multi-turn dialog between the user and the chatbot. This requires the chatbot to have the state (or “memory”) of all the previous turns to maintain the state of the conversation.\n",
        "\n",
        "In this tutorial, you'll learn about:\n",
        "- Creating a custom preamble\n",
        "- Creating a single-turn conversation\n",
        "- Building the conversation memory\n",
        "- Running a multi-turn conversation\n",
        "- Viewing the chat history\n",
        "\n",
        "You'll learn these by building an onboarding assistant for new hires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpF395muyw3N"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To get started, first we need to install the `cohere` library and create a Cohere client."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere"
      ],
      "metadata": {
        "id": "Vz1PrlCEz6Gg",
        "outputId": "1693d7cf-2e0a-4899-90fb-93c0974698c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.13.6-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.10.4)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Downloading cohere-5.13.6-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, parameterized, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.13.6 fastavro-1.10.0 httpx-sse-0.4.0 parameterized-0.9.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SMdscahAyw3N"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.ClientV2(api_key = userdata.get(\"COHERE_API_KEY\")) # Get your free API key: https://dashboard.cohere.com/api-keys"
      ],
      "metadata": {
        "id": "ac3qthlk09Kx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j311p9lsyw3O"
      },
      "source": [
        "## Creating a custom preamble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIy0s2NLyw3P"
      },
      "source": [
        "A conversation starts with a system message, or a preamble, to help steer a chatbot’s response toward certain characteristics.\n",
        "\n",
        "For example, if we want the chatbot to adopt a formal style, the preamble can be used to encourage the generation of more business-like and professional responses.\n",
        "\n",
        "The recommended approach is to use two H2 Markdown headers: \"Task and Context\" and \"Style Guide\" in the exact order.\n",
        "\n",
        "In the example below, the preamble provides context for the assistant's task (task and context) and encourages the generation of rhymes as much as possible (style guide)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hMB2MFttyw3P",
        "outputId": "c15f7d00-64e3-45ae-cc5f-2a4f66a184cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a little note, a friendly hello,\n",
            "To introduce yourself, a great way to go:\n",
            "\n",
            "\"Hello, fellow Co1t crew,\n",
            "My name is [Your Name], new to all of you.\n",
            "Excited to join this innovative team,\n",
            "Where ideas bloom and success is supreme.\n",
            "\n",
            "I'm eager to learn and contribute my part,\n",
            "Let's connect and create, a fresh start.\n",
            "Looking forward to meeting you all,\n",
            "And together, we'll make our vision tall.\"\n",
            "\n",
            "A simple rhyme, a friendly gesture,\n",
            "To begin your journey, a memorable adventure.\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "\n",
        "# Create a custom system message\n",
        "system_message=\"\"\"## Task and Context\n",
        "You are an assistant who assist new employees of Co1t with their first week.\n",
        "\n",
        "## Style Guide\n",
        "Try to speak in rhymes as much as possible. Be professional.\"\"\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [{\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfEJ6f6Zyw3Q"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on preambles](https://docs.cohere.com/docs/preambles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yol_RPNyw3R"
      },
      "source": [
        "## Starting the first conversation turn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbrggsMoyw3R"
      },
      "source": [
        "Let's start with the first conversation turn.\n",
        "\n",
        "Here, we are also adding a custom preamble or system message for generating a concise response, just to keep the outputs brief for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9ft25jjCyw3S",
        "outputId": "bc560444-af1a-41fa-cbcf-8f5067f85d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hi everyone! I'm thrilled to join Co1t today and look forward to collaborating with this talented team, let's create something amazing together!\"\n"
          ]
        }
      ],
      "source": [
        "# Add the user message\n",
        "message = \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"\n",
        "\n",
        "# Create a custom system message\n",
        "system_message=\"\"\"## Task and Context\n",
        "Generate concise responses, with maximum one-sentence.\"\"\"\n",
        "\n",
        "# Add the messages\n",
        "messages = [{\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6nTOlKPyw3S"
      },
      "source": [
        "## Building the conversation memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfF1P32Ryw3S"
      },
      "source": [
        "Now, we want the model to refine the earlier response. This requires the next generation to have access to the state, or memory, of the conversation.\n",
        "\n",
        "To do this, we append the `messages` with the model's previous response using the `assistant` role.\n",
        "\n",
        "Next, we also append a new user message (for the second turn) to the `messages` list.\n",
        "\n",
        "Looking at the response, we see that the model is able to get the context from the chat history. The model is able to capture that \"it\" in the user message refers to the introduction message it had generated earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77iESizTyw3T",
        "outputId": "d48f668c-9672-4283-ee53-4138418137d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Hey, future Co1t buddies! Stoked to join this awesome team, let's get to know each other and make some startup magic together!\"\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({'role' : 'assistant', 'content': response.message.content[0].text})\n",
        "\n",
        "# Add the user message\n",
        "message = \"Make it more upbeat and conversational.\"\n",
        "\n",
        "# Append the user message\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "# Generate the response with the current chat history as the context\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kuO3ME5yw3T"
      },
      "source": [
        "Further reading:\n",
        "- [Documentation on using the Chat endpoint](https://docs.cohere.com/docs/chat-api)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zk_TcNayw3T"
      },
      "source": [
        "## Running a multi-turn conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjGn1Gbvyw3T"
      },
      "source": [
        "\n",
        "You can continue doing this for any number of turns by continuing to append the chatbot's response and the new user message to the `messages` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1SePuptyw3T",
        "outputId": "75274af5-292c-4b4c-9776-227509c21213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Hi, boss! So excited to dive into my new role at Co1t and eager to learn from your mentorship and guidance. Let's crush it!\"\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "# Add the user message\n",
        "message = \"Thanks. Could you create another one for my DM to my manager.\"\n",
        "\n",
        "# Append the user message\n",
        "messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "# Generate the response with the current chat history as the context\n",
        "response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                   messages=messages)\n",
        "\n",
        "print(response.message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkdu8Odpyw3U"
      },
      "source": [
        "## Viewing the chat history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb76IfqGyw3U"
      },
      "source": [
        "To look at the current chat history, you can print the `messages` list, which contains a list of `user` and `assistant` turns in the same sequence as they were created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsv_pytDyw3U",
        "outputId": "02c5f64b-1f34-4aa8-8aeb-e0cbce7fff0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'system', 'content': '## Task and Context\\nGenerate concise responses, with maximum one-sentence.'} \n",
            "\n",
            "{'role': 'user', 'content': \"I'm joining a new startup called Co1t today. Could you help me write a short introduction message to my teammates.\"} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hello, teammates! I\\'m thrilled to join the Co1t family today and looking forward to getting to know you all and contributing to our shared success.\"'} \n",
            "\n",
            "{'role': 'user', 'content': 'Make it more upbeat and conversational.'} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hey, future Co1t buddies! Stoked to join this awesome team, let\\'s get to know each other and make some startup magic together!\"'} \n",
            "\n",
            "{'role': 'user', 'content': 'Thanks. Could you create another one for my DM to my manager.'} \n",
            "\n",
            "{'role': 'assistant', 'content': '\"Hi, boss! So excited to dive into my new role at Co1t and eager to learn from your mentorship and guidance. Let\\'s crush it!\"'} \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Append the previous response\n",
        "messages.append({\"role\": \"assistant\", \"content\": response.message.content[0].text})\n",
        "\n",
        "# View the chat history\n",
        "for message in messages:\n",
        "    print(message,\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR0RQd4hyw3U"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S74VVSnTyw3U"
      },
      "source": [
        "In this tutorial, you learned about:\n",
        "- How to create a custom preamble\n",
        "- How to create a single-turn conversation\n",
        "- How to build the conversation memory\n",
        "- How to run a multi-turn conversation\n",
        "- How to view the chat history\n",
        "\n",
        "You will use the same method for running a multi-turn conversation when you learn about other use cases such as RAG (Part 6) and tool use (Part 7).\n",
        "\n",
        "But to fully leverage these other capabilities, you will need another type of language model that generates text representations, or embeddings.\n",
        "\n",
        "In Part 4, you will learn how text embeddings can power an important use case for RAG, which is semantic search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw5_6-BKyw3V"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}